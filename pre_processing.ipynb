{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import tqdm\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from dataset_tools import OwnDataset, MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plotting import show_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/singh_cp_pipeline_singlecell_images\"\n",
    "\n",
    "folder_paths = glob.glob(os.path.join(data_path, \"*\"))\n",
    "\n",
    "len(folder_paths)\n",
    "\n",
    "# folder_paths\n",
    "\n",
    "subset_folder_paths = folder_paths[:50]\n",
    "\n",
    "# copy the first 50 folders to a new directory\n",
    "new_data_path = \"data_subset/singh_cp_pipeline_singlecell_images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for folder_path in subset_folder_paths:\n",
    "#     folder_name = folder_path.split(\"\\\\\")[-1]\n",
    "#     new_folder_path = os.path.join(new_data_path, folder_name)\n",
    "#     shutil.copytree(folder_path, new_folder_path)\n",
    "#     print(f\"Copying {folder_name} to {new_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "df_metadata = pd.read_parquet(\"metadata.parquet\")\n",
    "\n",
    "\n",
    "# combine all the npy files into a single numpy array\n",
    "npy_paths = glob.glob(os.path.join(new_data_path, \"*\", \"*.npy\"))\n",
    "len(npy_paths)\n",
    "\n",
    "file_1 = np.load(npy_paths[0])\n",
    "file_2 = np.load(npy_paths[1])\n",
    "\n",
    "total_data = np.array([])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_metadata[\"moa\"].iloc[0:1000].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_paths = npy_paths[0:5]\n",
    "lookup_paths = []\n",
    "\n",
    "for path in tmp_paths:\n",
    "    data = np.load(path)\n",
    "    lookup_path = path.split(\"\\\\\")[-1]\n",
    "    lookup_paths.append(lookup_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_metadata[df_metadata[\"Single_Cell_Image_Name\"].str.contains(\"B02_s1_w16F89C55C-7808-4136-82E4-E066F8E3CB10_0.npy\")]\n",
    "df_metadata[df_metadata[\"Single_Cell_Image_Name\"] == lookup_paths[0]][\"moa\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tqdm to see progress\n",
    "\n",
    "for i, npy_path in enumerate(tqdm.tqdm(npy_paths)):\n",
    "    data = np.load(npy_path)\n",
    "    data = (data / np.max(data) * 255).astype(np.int8)\n",
    "    lookup_path = path.split(\"\\\\\")[-1]\n",
    "    moa = df_metadata[df_metadata[\"Single_Cell_Image_Name\"] == lookup_path][\"moa\"].values[0]\n",
    "    # print(moa)\n",
    "    if i == 0:\n",
    "        total_data = np.expand_dims(data, axis=0)\n",
    "    else:\n",
    "        total_data = np.vstack((total_data, np.expand_dims(data, axis=0)))\n",
    "    if i == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(train=True, transform=transforms.ToTensor())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(loader))\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def npy_loader(path):\n",
    "    \"\"\"\n",
    "    load a npy file and change the dtype to int8\n",
    "    \"\"\"\n",
    "    sample = np.load(path)\n",
    "\n",
    "    sample = (sample / np.max(sample) * 255).astype(np.uint8)\n",
    "\n",
    "    sample = torch.from_numpy(sample)\n",
    "    # make channel the first dimension\n",
    "    sample = sample.permute(2, 0, 1)\n",
    "\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.DatasetFolder(\"data_subset/singh_cp_pipeline_singlecell_images\", loader=npy_loader, extensions=('.npy',))\n",
    "# dataset = OwnDataset()\n",
    "\n",
    "dataset[3050][0].shape\n",
    "show_img(dataset[3051][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_metadata = pd.read_csv(\"metadata.csv\", index_col=0)\n",
    "# df_features = pd.read_csv(\"CPfeatures.csv\", index_col=0) # very large file (4.2 GB)\n",
    "\n",
    "# read from parquet instead of csv\n",
    "\n",
    "# df_features = pd.read_parquet(\"CPfeatures.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.head()\n",
    "# df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata[\"Multi_Cell_Image_Id\"].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
