{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from dataset_tools import OwnDataset, make_train_test_val_split, make_small_subset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from models import LatentClassifier_2, VAE_CELL_CNN, CELL_CNN_CLASSIFIER\n",
    "from torchvision import transforms\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488396 1 1 1\n"
     ]
    }
   ],
   "source": [
    "tf = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# dataset = OwnDataset(transform=tf, path=r\"/Users/rasmusjensen/Documents/02466-Cell-Profiling/data_subset\")\n",
    "dataset = OwnDataset(transform=tf, path=r\"C:\\Users\\Otto\\Desktop\\Fagprojekt_data\\labelled_data\")\n",
    "train_subset, test_subset, val_subset = make_train_test_val_split(dataset)\n",
    "# train_subset = make_small_subset(dataset, 100000)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=len(train_subset), shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=len(test_subset), shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=len(val_subset), shuffle=True)\n",
    "\n",
    "print(len(dataset), len(train_loader), len(test_loader), len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"trained_models/\"\n",
    "\n",
    "\n",
    "latent_dim_2 = LatentClassifier_2(hidden_dim=4, latent_dim=2, num_classes=13)\n",
    "latent_dim_2.load_state_dict(torch.load(MODEL_DIR + f\"{latent_dim_2.__class__.__name__}_latent{latent_dim_2.latent_dim}_mean_best_model.pth\", map_location=torch.device('cpu')))\n",
    "latent_dim_2.eval()\n",
    "\n",
    "\n",
    "latent_dim_32 = LatentClassifier_2(hidden_dim=64, latent_dim=32, num_classes=13)\n",
    "latent_dim_32.load_state_dict(torch.load(MODEL_DIR + f\"{latent_dim_32.__class__.__name__}_latent{latent_dim_32.latent_dim}_mean_best_model.pth\", map_location=torch.device('cpu')))\n",
    "latent_dim_32.eval()\n",
    "\n",
    "\n",
    "latent_dim_256 = LatentClassifier_2(hidden_dim=512, latent_dim=256, num_classes=13)\n",
    "latent_dim_256.load_state_dict(torch.load(MODEL_DIR + f\"{latent_dim_256.__class__.__name__}_latent{latent_dim_256.latent_dim}_mean_best_model.pth\", map_location=torch.device('cpu')))\n",
    "latent_dim_256.eval()\n",
    "\n",
    "\n",
    "Vae_Cell_CNN_2 = VAE_CELL_CNN(input_dim=(3, 68, 68), hidden_dim=4, latent_dim=2)\n",
    "Vae_Cell_CNN_2.load_state_dict(torch.load(MODEL_DIR + f\"{Vae_Cell_CNN_2.__class__.__name__}_latent{Vae_Cell_CNN_2.latent_dim}_mean_best_model.pth\", map_location=torch.device('cpu')))\n",
    "Vae_Cell_CNN_2.eval()\n",
    "\n",
    "\n",
    "Vae_Cell_CNN_32 = VAE_CELL_CNN(input_dim=(3, 68, 68), hidden_dim=64, latent_dim=32)\n",
    "Vae_Cell_CNN_32.load_state_dict(torch.load(MODEL_DIR + f\"{Vae_Cell_CNN_32.__class__.__name__}_latent{Vae_Cell_CNN_32.latent_dim}_mean_best_model.pth\", map_location=torch.device('cpu')))\n",
    "Vae_Cell_CNN_32.eval()\n",
    "\n",
    "\n",
    "Vae_Cell_CNN_256 = VAE_CELL_CNN(input_dim=(3, 68, 68), hidden_dim=512, latent_dim=256)\n",
    "Vae_Cell_CNN_256.load_state_dict(torch.load(MODEL_DIR + f\"{Vae_Cell_CNN_256.__class__.__name__}_latent{Vae_Cell_CNN_256.latent_dim}_mean_best_model.pth\", map_location=torch.device('cpu')))\n",
    "Vae_Cell_CNN_256.eval()\n",
    "\n",
    "Cell_CNN_Classifier = CELL_CNN_CLASSIFIER(input_dim=(3, 68, 68), hidden_dim=128, num_classes=13)\n",
    "Cell_CNN_Classifier.load_state_dict(torch.load(MODEL_DIR + f\"{Cell_CNN_Classifier.__class__.__name__}_best_model.pth\", map_location=torch.device('cpu')))\n",
    "Cell_CNN_Classifier.eval()\n",
    "# \n",
    "models = [latent_dim_256, latent_dim_2, Cell_CNN_Classifier]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97680 97680\n"
     ]
    }
   ],
   "source": [
    "images, moas = next(iter(test_loader)) # this is the whole dataset, as batch size is the same as the dataset size\n",
    "print(len(images), len(moas))\n",
    "\n",
    "with torch.no_grad():\n",
    "    latent_output_256 = Vae_Cell_CNN_256(images)\n",
    "    pred_256 = latent_dim_256(latent_output_256[\"mu\"])\n",
    "    accuracy_latent_256 = (pred_256.argmax(1) == moas).cpu().numpy()\n",
    "\n",
    "    latent_output_2 = Vae_Cell_CNN_2(images)\n",
    "    pred_2 = latent_dim_2(latent_output_2[\"mu\"])\n",
    "    accuracy_latent_2 = (pred_2.argmax(1) == moas).cpu().numpy()\n",
    "\n",
    "    latent_output_32 = Vae_Cell_CNN_32(images)\n",
    "    pred_32 = latent_dim_32(latent_output_32[\"mu\"])\n",
    "    accuracy_latent_32 = (pred_32.argmax(1) == moas).cpu().numpy()\n",
    "\n",
    "    pred_CNN = Cell_CNN_Classifier(images)\n",
    "    accuracy_CNN = (pred_CNN.argmax(1) == moas).cpu().numpy()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7450143325143325,\n",
       " 0.6475634725634726,\n",
       " 0.7443079443079443,\n",
       " 0.8504811629811629)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracy_latent_256), np.mean(accuracy_latent_2), np.mean(accuracy_latent_32), np.mean(accuracy_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mcnemar(accuracy_1, accuracy_2):\n",
    "    # 2x2 confusion matrix for the two models\n",
    "    confus = confusion_matrix(accuracy_1, accuracy_2)\n",
    "\n",
    "    result = mcnemar(confus)\n",
    "\n",
    "    print(confus)\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    return result, confus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22215  2692]\n",
      " [12211 60562]]\n",
      "pvalue      0.0\n",
      "statistic   2692.0\n",
      "[[22046  2861]\n",
      " [ 2930 69843]]\n",
      "pvalue      0.3715494820511749\n",
      "statistic   2861.0\n",
      "[[12158 12749]\n",
      " [ 2447 70326]]\n",
      "pvalue      0.0\n",
      "statistic   2447.0\n"
     ]
    }
   ],
   "source": [
    "result, confus = compute_mcnemar(accuracy_latent_256, accuracy_latent_2)\n",
    "\n",
    "result, confus = compute_mcnemar(accuracy_latent_256, accuracy_latent_32)\n",
    "\n",
    "result, confus = compute_mcnemar(accuracy_latent_256, accuracy_CNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mcnemar(acc1, acc2):\n",
    "#     n00 = n01 = n10 = n11 = 0\n",
    "#     for i in range(len(acc1)):\n",
    "#         if acc1[i] and acc2[i]:\n",
    "#             n11 += 1\n",
    "#         elif acc1[i] and not acc2[i]:\n",
    "#             n10 += 1\n",
    "#         elif not acc1[i] and acc2[i]:\n",
    "#             n01 += 1\n",
    "#         else:\n",
    "#             n00 += 1\n",
    "#     confusion_matrix = [[n00, n01], [n10, n11]]\n",
    "#     print(f'Confusion Matrix: {confusion_matrix}')\n",
    "#     result = mcnemar(confusion_matrix, exact=True)\n",
    "#     print(f'statistic={result.statistic}, p-value={result.pvalue}')\n",
    "#     if result.pvalue < 0.05:\n",
    "#         print('Significant difference in performance (reject H0)')\n",
    "#     else:\n",
    "#         print('No significant difference in performance (fail to reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform McNemar's tests\n",
    "# print(\"CNN vs Latent 256\")\n",
    "# print(\"----------------\")\n",
    "# compute_mcnemar(batch_accuracy_CNN, batch_accuracy_256_latent)\n",
    "# print(\" \")\n",
    "\n",
    "# print(\"Latent 256 vs Latent 2\")\n",
    "# print(\"----------------\")\n",
    "# compute_mcnemar(batch_accuracy_256_latent, batch_accuracy_2_latent)\n",
    "# print(\" \")\n",
    "\n",
    "# print(\"Latent 2 vs CNN\")\n",
    "# print(\"----------------\")\n",
    "# compute_mcnemar(batch_accuracy_2_latent, batch_accuracy_CNN)\n",
    "# print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# def compute_mcnemar(acc1, acc2):\n",
    "#     n00 = n01 = n10 = n11 = 0\n",
    "#     for i in range(len(acc1)):\n",
    "#         if acc1[i] == 1 and acc2[i] == 1:\n",
    "#             n11 += 1\n",
    "#         elif acc1[i] == 1 and acc2[i] == 0:\n",
    "#             n10 += 1\n",
    "#         elif acc1[i] == 0 and acc2[i] == 1:\n",
    "#             n01 += 1\n",
    "#         else:\n",
    "#             n00 += 1\n",
    "#     confusion_matrix = [[n00, n01], [n10, n11]]\n",
    "#     print(f'Confusion Matrix: {confusion_matrix}')\n",
    "#     result = mcnemar(confusion_matrix, exact=True)\n",
    "#     print(f'statistic={result.statistic}, p-value={result.pvalue}')\n",
    "#     if result.pvalue < 0.05:\n",
    "#         print('Significant difference in performance (reject H0)')\n",
    "#     else:\n",
    "#         print('No significant difference in performance (fail to reject H0)')\n",
    "\n",
    "\n",
    "# #acc1 = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]  \n",
    "# #acc2 = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]  \n",
    "# #compute_mcnemar(acc1, acc2)\n",
    "\n",
    "# compute_mcnemar(batch_accuracy_CNN, batch_accuracy_256_latent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_mcnemar(batch_accuracy_CNN, batch_accuracy_256_latent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
